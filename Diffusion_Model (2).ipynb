{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "5d201d27",
      "metadata": {
        "id": "5d201d27",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import urllib\n",
        "import numpy as np\n",
        "import PIL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3f1e269c",
      "metadata": {
        "id": "3f1e269c",
        "scrolled": true
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m device \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m device\n",
            "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36b5ce8e",
      "metadata": {
        "id": "36b5ce8e",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "def get_sample_image()-> PIL.Image.Image:\n",
        "    url = 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTZmJy3aSZ1Ix573d2MlJXQowLCLQyIUsPdniOJ7rBsgG4XJb04g9ZFA9MhxYvckeKkVmo&usqp=CAU'\n",
        "    filename = 'racoon.jpg'\n",
        "    urllib.request.urlretrieve(url, filename)\n",
        "    return PIL.Image.open(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ba00d1d",
      "metadata": {
        "id": "4ba00d1d",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "def plot_noise_distribution(noise, predicted_noise):\n",
        "    plt.hist(noise.cpu().numpy().flatten(), density = True, alpha = 0.8, label = \"ground truth noise\")\n",
        "    plt.hist(predicted_noise.cpu().numpy().flatten(), density = True, alpha = 0.8, label = \"predicted noise\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0498796b",
      "metadata": {
        "id": "0498796b",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "IMAGE_SHAPE = (32, 32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fb68152",
      "metadata": {
        "id": "6fb68152",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize(IMAGE_SHAPE), # Resize the input image\n",
        "    transforms.ToTensor(), # Convert to torch tensor (scales data into [0,1])\n",
        "    transforms.Lambda(lambda t: (t * 2) - 1), # Scale data between [-1, 1]\n",
        "])\n",
        "\n",
        "\n",
        "reverse_transform = transforms.Compose([\n",
        "    transforms.Lambda(lambda t: (t + 1) / 2), # Scale data between [0,1]\n",
        "    transforms.Lambda(lambda t: t.permute(1, 2, 0)), # CHW to HWC\n",
        "    transforms.Lambda(lambda t: t * 255.), # Scale data between [0.,255.]\n",
        "    transforms.Lambda(lambda t: t.cpu().numpy().astype(np.uint8)), # Convert into an uint8 numpy array\n",
        "    transforms.ToPILImage(), # Convert to PIL image\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4937289f",
      "metadata": {
        "id": "4937289f",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "def plot_noise_prediction(noise, predicted_noise):\n",
        "    plt.figure(figsize=(15,15))\n",
        "    f, ax = plt.subplots(1, 2, figsize = (5,5))\n",
        "    ax[0].imshow(reverse_transform(noise))\n",
        "    ax[0].set_title(f\"ground truth noise\", fontsize = 10)\n",
        "    ax[1].imshow(reverse_transform(predicted_noise))\n",
        "    ax[1].set_title(f\"predicted noise\", fontsize = 10)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b031430",
      "metadata": {
        "id": "3b031430",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "class DiffusionModel:\n",
        "    def __init__(self, start_schedule=0.0001, end_schedule=0.02, timesteps = 1000):\n",
        "        self.start_schedule = start_schedule\n",
        "        self.end_schedule = end_schedule\n",
        "        self.timesteps = timesteps\n",
        "\n",
        "        \"\"\"\n",
        "        if\n",
        "            betas = [0.1, 0.2, 0.3, ...]\n",
        "        then\n",
        "            alphas = [0.9, 0.8, 0.7, ...]\n",
        "            alphas_cumprod = [0.9, 0.9 * 0.8, 0.9 * 0.8, * 0.7, ...]\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        self.betas = torch.linspace(start_schedule, end_schedule, timesteps)\n",
        "        self.alphas = 1 - self.betas\n",
        "        self.alphas_cumprod = torch.cumprod(self.alphas, axis=0)\n",
        "\n",
        "    def forward(self, x_0, t, device):\n",
        "        \"\"\"\n",
        "        x_0: (B, C, H, W)\n",
        "        t: (B,)\n",
        "        \"\"\"\n",
        "        noise = torch.randn_like(x_0, device=device)\n",
        "        sqrt_alphas_cumprod_t = self.get_index_from_list(self.alphas_cumprod.sqrt(), t, x_0.shape)\n",
        "        sqrt_one_minus_alphas_cumprod_t = self.get_index_from_list(torch.sqrt(1. - self.alphas_cumprod), t, x_0.shape)\n",
        "\n",
        "        mean = sqrt_alphas_cumprod_t * x_0\n",
        "        variance = sqrt_one_minus_alphas_cumprod_t * noise\n",
        "\n",
        "        return mean + variance, noise\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def backward(self, x, t, model, **kwargs):\n",
        "        \"\"\"\n",
        "        Calls the model to predict the noise in the image and returns\n",
        "        the denoised image.\n",
        "        Applies noise to this image, if we are not in the last step yet.\n",
        "        \"\"\"\n",
        "        betas_t = self.get_index_from_list(self.betas, t, x.shape)\n",
        "        sqrt_one_minus_alphas_cumprod_t = self.get_index_from_list(torch.sqrt(1. - self.alphas_cumprod), t, x.shape)\n",
        "        sqrt_recip_alphas_t = self.get_index_from_list(torch.sqrt(1.0 / self.alphas), t, x.shape)\n",
        "        mean = sqrt_recip_alphas_t * (x - betas_t * model(x, t, **kwargs) / sqrt_one_minus_alphas_cumprod_t)\n",
        "        posterior_variance_t = betas_t\n",
        "\n",
        "        if t == 0:\n",
        "            return mean\n",
        "        else:\n",
        "            noise = torch.randn_like(x, device=x.device)\n",
        "            variance = torch.sqrt(posterior_variance_t) * noise\n",
        "            return mean + variance\n",
        "\n",
        "    @staticmethod\n",
        "    def get_index_from_list(values, t, x_shape):\n",
        "        batch_size = t.shape[0]\n",
        "        \"\"\"\n",
        "        pick the values from vals\n",
        "        according to the indices stored in `t`\n",
        "        \"\"\"\n",
        "        result = values.gather(-1, t.cpu())\n",
        "        \"\"\"\n",
        "        if\n",
        "        x_shape = (5, 3, 64, 64)\n",
        "            -> len(x_shape) = 4\n",
        "            -> len(x_shape) - 1 = 3\n",
        "\n",
        "        and thus we reshape `out` to dims\n",
        "        (batch_size, 1, 1, 1)\n",
        "\n",
        "        \"\"\"\n",
        "        return result.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a1d3f02",
      "metadata": {
        "id": "4a1d3f02",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "pil_image = get_sample_image()\n",
        "torch_image = transform(pil_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a905d73",
      "metadata": {
        "id": "0a905d73",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "diffusion_model = DiffusionModel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2ba3c9d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "id": "e2ba3c9d",
        "outputId": "e84b294d-ecf3-4c64-a02b-6458f399181b",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "NO_DISPLAY_IMAGES = 5\n",
        "torch_image_batch = torch.stack([torch_image] * NO_DISPLAY_IMAGES)\n",
        "torch_image_batch = torch_image_batch.to(device)\n",
        "t = torch.linspace(0, diffusion_model.timesteps - 1, NO_DISPLAY_IMAGES).long()\n",
        "noisy_image_batch, _ = diffusion_model.forward(torch_image_batch, t, device)\n",
        "\n",
        "plt.figure(figsize=(15,15))\n",
        "f, ax = plt.subplots(1, NO_DISPLAY_IMAGES, figsize = (100,100))\n",
        "\n",
        "for idx, image in enumerate(noisy_image_batch):\n",
        "    ax[idx].imshow(reverse_transform(image))\n",
        "    ax[idx].set_title(f\"Iteration: {t[idx].item()}\", fontsize = 100)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4dfe406",
      "metadata": {
        "id": "d4dfe406"
      },
      "outputs": [],
      "source": [
        "class SinusoidalPositionEmbeddings(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, time):\n",
        "        device = time.device\n",
        "        half_dim = self.dim // 2\n",
        "        embeddings = math.log(10000) / (half_dim - 1)\n",
        "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
        "        embeddings = time[:, None] * embeddings[None, :]\n",
        "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
        "        return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e65d6453",
      "metadata": {
        "id": "e65d6453",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, channels_in, channels_out, time_embedding_dims, labels, num_filters = 3, downsample=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.time_embedding_dims = time_embedding_dims\n",
        "        self.time_embedding = SinusoidalPositionEmbeddings(time_embedding_dims)\n",
        "        self.labels = labels\n",
        "        if labels:\n",
        "            self.label_mlp = nn.Linear(1, channels_out)\n",
        "\n",
        "        self.downsample = downsample\n",
        "\n",
        "        if downsample:\n",
        "            self.conv1 = nn.Conv2d(channels_in, channels_out, num_filters, padding=1)\n",
        "            self.final = nn.Conv2d(channels_out, channels_out, 4, 2, 1)\n",
        "        else:\n",
        "            self.conv1 = nn.Conv2d(2 * channels_in, channels_out, num_filters, padding=1)\n",
        "            self.final = nn.ConvTranspose2d(channels_out, channels_out, 4, 2, 1)\n",
        "\n",
        "        self.bnorm1 = nn.BatchNorm2d(channels_out)\n",
        "        self.bnorm2 = nn.BatchNorm2d(channels_out)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(channels_out, channels_out, 3, padding=1)\n",
        "        self.time_mlp = nn.Linear(time_embedding_dims, channels_out)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x, t, **kwargs):\n",
        "        o = self.bnorm1(self.relu(self.conv1(x)))\n",
        "        o_time = self.relu(self.time_mlp(self.time_embedding(t)))\n",
        "        o = o + o_time[(..., ) + (None, ) * 2]\n",
        "        if self.labels:\n",
        "            label = kwargs.get('labels')\n",
        "            o_label = self.relu(self.label_mlp(label))\n",
        "            o = o + o_label[(..., ) + (None, ) * 2]\n",
        "\n",
        "        o = self.bnorm2(self.relu(self.conv2(o)))\n",
        "\n",
        "        return self.final(o)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ec1c137",
      "metadata": {
        "id": "2ec1c137"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, img_channels = 3, time_embedding_dims = 128, labels = False, sequence_channels = (96, 144, 386, 512, 768)):\n",
        "        super().__init__()\n",
        "        self.time_embedding_dims = time_embedding_dims\n",
        "        sequence_channels_rev = reversed(sequence_channels)\n",
        "\n",
        "        self.downsampling = nn.ModuleList([Block(channels_in, channels_out, time_embedding_dims, labels) for channels_in, channels_out in zip(sequence_channels, sequence_channels[1:])])\n",
        "        self.upsampling = nn.ModuleList([Block(channels_in, channels_out, time_embedding_dims, labels,downsample=False) for channels_in, channels_out in zip(sequence_channels[::-1], sequence_channels[::-1][1:])])\n",
        "        self.conv1 = nn.Conv2d(img_channels, sequence_channels[0], 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(sequence_channels[0], img_channels, 1)\n",
        "\n",
        "\n",
        "    def forward(self, x, t, **kwargs):\n",
        "        residuals = []\n",
        "        o = self.conv1(x)\n",
        "        for ds in self.downsampling:\n",
        "            o = ds(o, t, **kwargs)\n",
        "            residuals.append(o)\n",
        "        for us, res in zip(self.upsampling, reversed(residuals)):\n",
        "            o = us(torch.cat((o, res), dim=1), t, **kwargs)\n",
        "\n",
        "        return self.conv2(o)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b74d1f78",
      "metadata": {
        "id": "b74d1f78",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128\n",
        "NO_EPOCHS = 100\n",
        "PRINT_FREQUENCY = 1\n",
        "LR = 0.001\n",
        "VERBOSE = False\n",
        "\n",
        "unet = UNet(labels=True)\n",
        "unet.to(device)\n",
        "optimizer = torch.optim.Adam(unet.parameters(), lr=LR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74cb4d2c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74cb4d2c",
        "outputId": "95a7eaa4-d27d-4132-8f5b-a64f98932f28",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=False, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, drop_last=True, pin_memory=True)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=False, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, drop_last=True, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6be6b824",
      "metadata": {
        "id": "6be6b824",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "for epoch in range(NO_EPOCHS):\n",
        "\n",
        "    mean_epoch_loss = torch.tensor([], device=device)\n",
        "    mean_epoch_loss_val = torch.tensor([], device=device)\n",
        "\n",
        "    for batch, label in trainloader:\n",
        "        t = torch.randint(0, diffusion_model.timesteps, (BATCH_SIZE,), dtype=int, device=device)\n",
        "        batch_noisy, noise = diffusion_model.forward(batch.to(device), t, device)\n",
        "        predicted_noise = unet(batch_noisy, t, labels = label.reshape(-1,1).float().to(device))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = torch.nn.functional.l1_loss(noise, predicted_noise)\n",
        "        mean_epoch_loss = torch.cat((mean_epoch_loss, loss.detach().unsqueeze(0)), dim=0)\n",
        "        loss.backward()\n",
        "        optimizer.step()    \n",
        "\n",
        "    for batch, label in testloader:\n",
        "    \n",
        "        t = torch.randint(0, diffusion_model.timesteps, (BATCH_SIZE,), dtype=int, device=device)\n",
        "        batch_noisy, noise = diffusion_model.forward(batch.to(device), t, device)\n",
        "        predicted_noise = unet(batch_noisy, t, labels = label.reshape(-1,1).float().to(device))\n",
        "\n",
        "        loss = torch.nn.functional.l1_loss(noise, predicted_noise)\n",
        "        mean_epoch_loss_val = torch.cat((mean_epoch_loss, loss.detach().unsqueeze(0)), dim=0)\n",
        "\n",
        "    if epoch % PRINT_FREQUENCY == 0:\n",
        "        print('---')\n",
        "        print(f\"Epoch: {epoch} | Train Loss {torch.mean(mean_epoch_loss)} | Val Loss {torch.mean(mean_epoch_loss_val)}\")\n",
        "\n",
        "        torch.save(unet.state_dict(), f\"epoch: {epoch}\")\n",
        "    del loss\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b9d4a24",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b9d4a24",
        "outputId": "018d1d3e-0dde-43bc-b375-3400d2cdfbc8"
      },
      "outputs": [],
      "source": [
        "unet = UNet(labels=True)\n",
        "unet.load_state_dict(torch.load((\"epoch: 15\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b893eca5",
      "metadata": {
        "id": "b893eca5"
      },
      "outputs": [],
      "source": [
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a5cb534",
      "metadata": {
        "id": "7a5cb534"
      },
      "outputs": [],
      "source": [
        "NUM_CLASSES = len(classes)\n",
        "NUM_DISPLAY_IMAGES = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25aa26b1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "25aa26b1",
        "outputId": "6eb09089-d427-45f3-fe68-6aab5f3be3c5",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(10)\n",
        "\n",
        "plt.figure(figsize=(15,15))\n",
        "f, ax = plt.subplots(NUM_CLASSES, NUM_DISPLAY_IMAGES, figsize = (100,100))\n",
        "\n",
        "for c in range(NUM_CLASSES):\n",
        "    imgs = torch.randn((NUM_DISPLAY_IMAGES, 3) + IMAGE_SHAPE).to(device)\n",
        "    for i in reversed(range(diffusion_model.timesteps)):\n",
        "        t = torch.full((1,), i, dtype=torch.long, device=device)\n",
        "        labels = torch.tensor([c] * NUM_DISPLAY_IMAGES).resize(NUM_DISPLAY_IMAGES, 1).float().to(device)\n",
        "        imgs = diffusion_model.backward(x=imgs, t=t, model=unet.eval().to(device), labels = labels)\n",
        "    for idx, img in enumerate(imgs):\n",
        "        ax[c][idx].imshow(reverse_transform(img))\n",
        "        ax[c][idx].set_title(f\"Class: {classes[c]}\", fontsize = 100)\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
