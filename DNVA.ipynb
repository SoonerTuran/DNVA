{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOu8ahAbq5f2OAKG5/Oi0t6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SoonerTuran/DNVA/blob/main/DNVA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "HHTQmlFe7lWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "l-R76_tkAevP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtIaVK7U7aG9"
      },
      "outputs": [],
      "source": [
        "class ALU(nn.Module):\n",
        "    def __init__(self, input_size, opcode_size, output_size, flag_size, hidden_size=32):\n",
        "        super(ALU, self).__init__()\n",
        "\n",
        "        # Layers definition\n",
        "        self.linear1 = nn.Linear(self.input_size + self.opcode_size, hidden_size)\n",
        "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.linear3 = nn.Linear(hidden_size, self.output_size + flag_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.sin(self.linear1(x))\n",
        "        x = torch.sin(self.linear2(x))\n",
        "        x = torch.sin(self.linear3(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        output_size = 2 ** input_size\n",
        "        self.fc1 = nn.Linear(input_size, input_size*2)\n",
        "        self.fc2 = nn.Linear(input_size*2, input_size*3)\n",
        "        self.fc3 = nn.Linear(input_size*3, input_size*4)\n",
        "        self.fc4 = nn.Linear(input_size*4, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = torch.sin(self.fc1(x))\n",
        "      x = torch.sin(self.fc2(x))\n",
        "      x = torch.sin(self.fc3(x))\n",
        "      x = torch.sin(self.fc4(x))\n",
        "      return x"
      ],
      "metadata": {
        "id": "e2q6ivozzc1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "input_size = 13\n",
        "num_samples = 2 ** input_size\n",
        "output_size = 2 ** input_size\n",
        "\n",
        "# Tüm olası ikili sayıları oluşturmak için itertools.product kullanılıyor.\n",
        "binary_values = list(itertools.product([0, 1], repeat=input_size))\n",
        "# Toplam ikili sayı sayısı num_samples'tan fazla ise, sadece ilk num_samples kadarını alırız.\n",
        "binary_values = binary_values[:num_samples]\n",
        "\n",
        "# Liste halindeki ikili sayıları tensörlere dönüştürme\n",
        "train_inputs = torch.tensor(binary_values, dtype=torch.int, device=device , requires_grad=False)\n",
        "test_inputs = torch.randint(0, 2, (num_samples, input_size), device=device, requires_grad=False)\n",
        "\n",
        "# One-hot çıktıları oluşturma\n",
        "train_outputs = torch.zeros(num_samples, output_size, device=device)\n",
        "test_outputs = torch.zeros(num_samples, output_size, device=device)\n",
        "for i in range(num_samples):\n",
        "    # Her ikili sayıyı ondalık olarak dönüştürme\n",
        "    decimal1 = int(''.join(map(str, train_inputs[i].tolist())), 2)\n",
        "    decimal2 = int(''.join(map(str, test_inputs[i].tolist())), 2)\n",
        "    # Karşılık gelen indekse 1 atama\n",
        "    train_outputs[i][decimal1] = 1\n",
        "    test_outputs[i][decimal2] = 1\n",
        "\n",
        "train_inputs = train_inputs.float()\n",
        "test_inputs = test_inputs.float()\n",
        "train_outputs = train_outputs.float()\n",
        "test_outputs = test_outputs.float()\n",
        "\n",
        "del binary_values\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "d_V5RyHV_Pdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Decoder(input_size=input_size)\n",
        "model.to(device)\n",
        "\n",
        "# Setup a loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Setup an optimizer (stochastic gradient descent)\n",
        "optimizer = torch.optim.Adam(params = model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "4LfP6Vi074az"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(y_pred, y_true):\n",
        "    _, predicted = torch.max(y_pred.data, 1)\n",
        "    correct = (predicted == torch.argmax(y_true, 1)).sum().item()\n",
        "    accuracy = correct / y_true.size(0)\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "epochs = 20000\n",
        "\n",
        "# Track different values\n",
        "epoch_count = []\n",
        "loss_values = []\n",
        "test_loss_values = []\n",
        "accuracy_values = []\n",
        "test_accuracy_values = []\n",
        "\n",
        "#Training\n",
        "for epoch in range(epochs):\n",
        "  # Set the model to training mode\n",
        "  model.train() # train mode in PyTorch sets all parameters that require gradients to require gradients\n",
        "\n",
        "  # 1. Forward pass\n",
        "  y_pred = model(train_inputs)\n",
        "\n",
        "  # 2. Calculate the loss\n",
        "  loss = loss_fn(y_pred, train_outputs)\n",
        "\n",
        "  # 3. Optimizer zero grad\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # 4. Perform backpropagation on the loss with respect to the parameters of the model (calculate gradients of each parameter)\n",
        "  loss.backward()\n",
        "\n",
        "  # 5. Step the optimizer (perform gradient descent)\n",
        "  optimizer.step() # by default how the optimizer changes will acculumate through the loop so... we have to zero them above in step 3 for the next iteration of the loop\n",
        "\n",
        "  # Calculate training accuracy\n",
        "  accuracy = calculate_accuracy(y_pred, train_outputs)\n",
        "\n",
        "  ### Testing\n",
        "  model.eval() # turns off different settings in the model not needed for evaluation/testing (dropout/batch norm layers)\n",
        "  with torch.inference_mode(): # turns off gradient tracking & a couple more things behind the scenes\n",
        "    # 1. Do the forward pass\n",
        "    test_pred = model(test_inputs)\n",
        "\n",
        "    # 2. Calculate the loss\n",
        "    test_loss = loss_fn(test_pred, test_outputs)\n",
        "\n",
        "    # Calculate testing accuracy\n",
        "    test_accuracy = calculate_accuracy(test_pred, test_outputs)\n",
        "\n",
        "  # Print out what's happenin'\n",
        "  if epoch % 100  == 0:\n",
        "    epoch_count.append(epoch)\n",
        "    loss_values.append(loss.detach().item())\n",
        "    test_loss_values.append(test_loss.detach().item())\n",
        "\n",
        "    accuracy_values.append(accuracy)\n",
        "    test_accuracy_values.append(test_accuracy)\n",
        "\n",
        "    print(f\"Epoch: {epoch} | Loss: {loss.detach().item():.6f} | Test loss: {test_loss.detach().item():.6f} | Train Accuracy: {accuracy:.6f} | Test Accuracy: {test_accuracy:.6f}\")\n",
        "\n",
        "  del loss, test_loss, y_pred\n",
        "  torch.cuda.empty_cache()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "GmP6wpIw9XgQ",
        "outputId": "6072a610-5032-45d6-b3a7-cd396ac03f30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-ded830629f16>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0;31m# 2. Calculate the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0;31m# 3. Optimizer zero grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1175\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                label_smoothing=self.label_smoothing)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3028\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3029\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 4.00 GiB (GPU 0; 14.75 GiB total capacity; 12.05 GiB already allocated; 1.91 GiB free; 12.06 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the loss curves\n",
        "plt.plot(epoch_count, np.array(torch.tensor(loss_values)), label=\"Train loss\")\n",
        "plt.plot(epoch_count, np.array(torch.tensor(test_loss_values)), label=\"Test loss\")\n",
        "plt.title(\"Training and test loss curves\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.legend();\n",
        "plt.show()\n",
        "\n",
        "# Plot the loss curves\n",
        "plt.plot(epoch_count, np.array(torch.tensor(accuracy_values)), label=\"Train Acc\")\n",
        "plt.plot(epoch_count, np.array(torch.tensor(test_loss_values)), label=\"Test Acc\")\n",
        "plt.title(\"Training and test acc curves\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.legend();\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_rNTkPzO_e3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "  test_pred = model(test_inputs)\n",
        "\n",
        "index = torch.randint(low=0, high=512, size=(1,1))[0][0]\n",
        "print(test_inputs[index])\n",
        "\n",
        "print(test_pred[index].round(decimals=2))\n",
        "print(test_pred[index].round(decimals=2).argmax())\n",
        "\n",
        "# Binary tensoru bir stringe dönüştürelim\n",
        "binary_str = ''.join([str(int(i)) for i in test_inputs[index]])\n",
        "\n",
        "# Binary stringi desimal sayıya dönüştürelim\n",
        "decimal_number = int(binary_str, 2)\n",
        "print(decimal_number)"
      ],
      "metadata": {
        "id": "jopNuH_Pc0wZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}